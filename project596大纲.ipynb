{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byOwXnSNKJWU"
   },
   "source": [
    "# 基于字典学习的分类的实现\n",
    "\n",
    "假定对于某一大类的数据(例如 人脸图像)，我们有$k$个类别的数据（$k$个人），给定任意一个未知类别的样本通过SVM判断出其类别。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQlKB15dYqfs"
   },
   "source": [
    "Implementation of classification based on dictionary learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT_u7qosYoVQ"
   },
   "source": [
    "Assume that for a certain large category of data (such as face images), we have $k$ categories of data ($k$ individuals), and given any sample of an unknown category, its category can be determined through SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os7QYyQ5spCQ"
   },
   "source": [
    "## 数据预处理：特征中心化\n",
    "$$\\vec{y}_t=\\vec{x}_t - \\frac{1}{N_t} \\sum_{t=1}^{N_t}\\vec{x}_t$$\n",
    "$$\\text{Data: }\\left\\{\\cdots|\\vec{x}_t|\\cdots\\right\\} ~\\&~ Y = \\left\\{\\cdots|\\vec{y}_t|\\cdots\\right\\},~t\\in(1,N_t) $$\n",
    "* 可以消除特征的偏置，这有助于防止模型在学习室过度依赖某个特征的绝对值。\n",
    "* 可以简化学习过程，使得模型更容易捕捉数据的模式。\n",
    "\n",
    "数据的特征中心化是指通过减去每个特征的均值，将数据的特征值调整到以零为中心。这个过程可以有助于消除不同特征之间的尺度差异，使数据更容易处理和分析。中心化后的数据具有零均值，即每个特征的平均值都变成了零。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_gGvnY3tFRr"
   },
   "source": [
    "## Data preprocessing: eigenvalue centralization\n",
    "$$\\vec{y}_t=\\vec{x}_t - \\frac{1}{N_t} \\sum_{t=1}^{N_t}\\vec{x}_t$$\n",
    "$$\\text{Data: }\\left\\{\\cdots|\\vec{x}_t|\\cdots\\right\\} ~\\&~ Y = \\left\\{\\cdots|\\vec{y}_t|\\cdots \\right\\},~t\\in(1,N_t) $$\n",
    "* Can de-bias features, which helps prevent the model from overly relying on the absolute value of a feature in the learning room.\n",
    "* It can simplify the learning process and make it easier for the model to capture data patterns.\n",
    "\n",
    "Eigenvalue centering of data refers to adjusting the eigenvalues of data to be centered around zero by subtracting the mean of each feature. This process can help eliminate scale differences between different features, making the data easier to process and analyze. Centered data has zero mean, that is, the mean of each feature becomes zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFrZbgfg0woq"
   },
   "source": [
    "# 算法逻辑：\n",
    "\n",
    "* 字典学习：\n",
    "\n",
    "  使用训练集的数据矩阵进行字典学习。字典学习的目的是学习一组基向量形成的矩阵，这些基向量可以有效地通过稀疏表示向量表示原始数据。\n",
    "\n",
    "* SVM 分类：\n",
    "\n",
    "  使用 SVM 对特征矩阵进行训练。在这里，每个样本是一个图像的稀疏编码，而标签是该图像所属的类别。训练完成后，SVM 模型学到了如何将稀疏编码映射到对应的类别。\n",
    "\n",
    "* 分类阶段：\n",
    "\n",
    "  对于测试集中的新图像，使用已经学到的字典，计算新图像的稀疏表示向量。将新图像的稀疏表示向量输入到训练好的 SVM 模型中，进行分类预测。SVM 将输出该图像所属的类别。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNbDKwEfby2H"
   },
   "source": [
    "# Algorithm logic:\n",
    "\n",
    "* Dictionary learning:\n",
    "\n",
    "   Dictionary learning using the data matrix of the training set. The purpose of dictionary learning is to learn a matrix formed by a set of basis vectors that can effectively represent the original data through a sparse representation vector.\n",
    "\n",
    "* SVM classification:\n",
    "\n",
    "   Use SVM to train the feature matrix. Here, each sample is a sparse representation vector of an image, and the label is the category to which the image belongs. After training, the SVM model learns how to map sparse encodings to corresponding categories.\n",
    "\n",
    "* Classification stage:\n",
    "\n",
    "   For a new image in the test set, use the learned dictionary to calculate a sparse representation vector of the new image. The sparse representation vector of the new image is input into the trained SVM model to make classification predictions. The SVM will output the category to which the image belongs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGGznjcr29ZH"
   },
   "source": [
    "## 字典的概念：\n",
    "通过选择或学习的字典（$A$），使得给定的观测数据集($Y$)可以被稀疏表示为稀疏表示矩阵（$Z$）。其中$Z$的每一列是对应样本的稀疏表示向量。\n",
    "$$\n",
    "A_{(n\\times p)}Z_{(p\\times m)} =\\hat{Y}_{(n\\times m)} ≈ Y_{(n\\times m)}\n",
    "$$\n",
    "$n$ 是行数，意味着参数数量，例如图像的平铺像素，$m$，时列数，意味着样本个数。\n",
    "我们期望 $n<p<m$ Underdetermind。这样Z有无数多个解。如果过定会有0个解。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNNfPPfkdK2j"
   },
   "source": [
    "## The concept of dictionary:\n",
    "By selecting or learning a dictionary ($A$), a given observation data set ($Y$) can be sparsely represented as a sparse representation matrix ($Z$). Each column of $Z$ is a sparse representation vector of the corresponding sample.\n",
    "$$\n",
    "A_{(n\\times p)}Z_{(p\\times m)} =\\hat{Y}_{(n\\times m)} ≈ Y_{(n\\times m)}\n",
    "$$\n",
    "$n$ is the number of rows, which means the number of parameters, such as the tiled pixels of the image, and $m$, the number of columns, means the number of samples.\n",
    "We expect $n<p<m$ Underdetermind. In this way, Z has countless solutions. If it is overdetermined there will be 0 solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIoC-hYa-AdO"
   },
   "source": [
    "## 特征提取\n",
    "在特征提取的过程中，也就是稀疏表示向量，字典有两种获取方式\n",
    "\n",
    "1. universal basis：\n",
    "  例如傅里叶转换，小波转换等。\n",
    "2. Tailored Basis：\n",
    "  通过主成分分析，（svd分解）获得的主成分矩阵$U$作为初始字典$A_0$，然后进行迭代寻找最优$A,Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t28JJEBBemkb"
   },
   "source": [
    "## Feature extraction\n",
    "In the process of feature extraction, that is, sparse representation vectors, there are two ways to obtain the dictionary.\n",
    "\n",
    "1.universal basis:\n",
    "   For example, Fourier transform, wavelet transform, etc.\n",
    "2. Tailored Basis:\n",
    "   Through principal component analysis, the principal component matrix $U$ obtained (svd decomposition) is used as the initial dictionary $A_0$, and then iteration is performed to find the optimal $A, Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0kx9fYS-PmG"
   },
   "source": [
    "### Universal basis\n",
    "对于通用基底来说，Z一定是稀疏的（章节III）。\n",
    "\n",
    "* 优点：\n",
    "\n",
    "  节约大量计算成本，通用性强。\n",
    "\n",
    "* 缺点：\n",
    "\n",
    "  不够灵活，性能不佳，数据分布差异无法适应。\n",
    "\n",
    "算法：\n",
    "$$\n",
    "\\arg_{Z}\\min \\|Y-AZ\\|_F^2\n",
    "$$\n",
    "我们将找到对于观测矩阵与预测矩阵的F范数的最小值的稀疏表示矩阵$Z$。\n",
    "\n",
    "Frobenius 范数实际上是矩阵元素的平方和的平方根。它量化了矩阵中所有元素的总体大小。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Lg19Fh-fWFc"
   },
   "source": [
    "### Universal basis\n",
    "For universal bases, Z must be sparse (Section III).\n",
    "\n",
    "* advantage:\n",
    "\n",
    "   Save a lot of computing costs and have strong versatility.\n",
    "\n",
    "* Disadvantages:\n",
    "\n",
    "   It is not flexible enough, performs poorly, and cannot adapt to differences in data distribution.\n",
    "\n",
    "algorithm:\n",
    "$$\n",
    "\\arg_{Z}\\min \\|Y-AZ\\|_F^2\n",
    "$$\n",
    "We will find the sparse representation matrix $Z$ that has the minimum value of the F-norm of the observation matrix and the prediction matrix.\n",
    "\n",
    "The Frobenius norm is actually the square root of the sum of the squares of the matrix elements. It quantifies the overall size of all elements in the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCYiNzhj_7PH"
   },
   "source": [
    "### Tailored Basis\n",
    "\n",
    "对于定制基底来说，$Z$不一定是稀疏的。\n",
    "\n",
    "* 优点：\n",
    "\n",
    "  能更好的捕捉数据结构与特征。\n",
    "\n",
    "* 缺点：\n",
    "\n",
    "  计算成本极大！！！例如SVD的计算成本是n^3.\n",
    "\n",
    "算法：\n",
    "\n",
    "  * 计算初始字典$A_0$（SVD $\\to$ $U$）;\n",
    "\n",
    "  * 如果 $\\|Y-AZ_{j+1}\\|_F^2$不够小：\n",
    "\n",
    "    * 找到\n",
    "$Z_{j+1} = \\arg_{Z}\\min \\| Y-A_jZ \\|_F^2$;\n",
    "\n",
    "    * 对于\n",
    "$Z_{j+1}$, 找到 $A_{j+1}\\arg_{A}\\min \\|Y-AZ_{j+1}\\|_F^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1DGMPLVgg88"
   },
   "source": [
    "### Tailored Basis\n",
    "\n",
    "For custom substrates, $Z$ is not necessarily sparse.\n",
    "\n",
    "* advantage:\n",
    "\n",
    "   It can better capture data structure and characteristics.\n",
    "\n",
    "* Disadvantages:\n",
    "\n",
    "   The computational cost is huge! ! ! For example, the computational cost of SVD is n^3.\n",
    "\n",
    "algorithm:\n",
    "\n",
    "   * Calculate the initial dictionary $A_0$ (SVD $\\to$ $U$);\n",
    "\n",
    "   * If $\\|Y-AZ_{j+1}\\|_F^2$ is not small enough:\n",
    "\n",
    "     * Find\n",
    "  $Z_{j+1} = \\arg_{Z}\\min \\| Y-A_jZ \\|_F^2$;\n",
    "\n",
    "     * for\n",
    "$Z_{j+1}$, find $A_{j+1}\\arg_{A}\\min \\|Y-AZ_{j+1}\\|_F^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDVAjnFGFOV9"
   },
   "source": [
    "为了确保稀疏性，我们引入正则化。\n",
    "$$\\|Y-AZ\\|_F^2+ λ\\|Z\\|_1$$\n",
    "\n",
    "促使模型参数更加平滑，使得其中一些参数更趋向于0。有助于提高泛化能力，防止过度拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ50D8KWjWQy"
   },
   "source": [
    "To ensure sparsity, we introduce regularization.\n",
    "$$\\|Y-AZ\\|_F^2+ λ\\|Z\\|_1$$\n",
    "\n",
    "Make the model parameters smoother, making some of them closer to 0. Helps improve generalization ability and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ue-P4pwYF_8U"
   },
   "source": [
    "为了找到$\\arg\\min\\|Y-AZ\\|_F^2+ λ\\|Z\\|_1$ 我们设$\\nabla \\|Y-AZ\\|_F^2=0$.\n",
    "\n",
    "通过计算，当$A = YZ^T(ZZ^T)^{-1}=YZ^{-p}$时，$∇\\|.\\|=0$.\n",
    "\n",
    "对$Z$进行SVD分解，我们有\n",
    "$$\n",
    "A=YV_Z\\Sigma_Z^{-1}U_Z^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wx92rWVjuCV"
   },
   "source": [
    "To find $\\arg\\min\\|Y-AZ\\|_F^2+ λ\\|Z\\|_1$ we set $\\nabla \\|Y-AZ\\|_F^2=0$.\n",
    "\n",
    "By calculation, when $A = YZ^T(ZZ^T)^{-1}=YZ^{-p}$, $∇\\|.\\|=0$.\n",
    "\n",
    "Performing SVD decomposition on $Z$, we have\n",
    "$$\n",
    "A=YV_Z\\Sigma_Z^{-1}U_Z^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfZ_W5rVJ0Qt"
   },
   "source": [
    "# 支持向量机，SVM\n",
    "支持向量机（Support Vector Machine，SVM）是一种强大的监督学习算法，广泛用于分类和回归问题。SVM 的分类原理可以简要概括如下：\n",
    "\n",
    "* SVM 的目标是在给定的训练数据集中找到一个决策边界（超平面），将不同类别的样本分隔开。\n",
    "\n",
    "* SVM 通过最大化分类间隔来找到决策边界。分类间隔是指离超平面最近的特征稀疏向量到超平面的距离。SVM 的目标是找到使得分类间隔最大的超平面。\n",
    "\n",
    "* 分类间隔边界上的特征稀疏向量被称为支持向量。在训练 SVM 时，只有支持向量的位置和数量对最终分类超平面的确定有影响。\n",
    "\n",
    "* SVM 可以使用核函数将非线性问题映射到高维特征空间，从而在高维空间中找到一个线性的超平面。常用的核函数包括线性核、多项式核和径向基函数（RBF）核。\n",
    "\n",
    "* SVM 的分类问题可以被转化为一个凸优化问题。通过最小化损失函数，SVM 找到一个最优的超平面，使得分类间隔最大。\n",
    "\n",
    "SVM 最初设计用于二分类问题，但可以通过一些策略扩展到多类别问题。如一对一(OvO), 会构建 $C(40,2)$ 个 SVM 分类器，每个分类器解决两个类别之间的问题。一对其余(OvR),对于每个类别，构建一个 SVM 分类器，用于将该类别与所有其他类别区分开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVwEEw-FkkQ7"
   },
   "source": [
    "# Support vector machine, SVM\n",
    "Support Vector Machine (SVM) is a powerful supervised learning algorithm widely used in classification and regression problems. The classification principle of SVM can be briefly summarized as follows:\n",
    "\n",
    "* The goal of SVM is to find a decision boundary (hyperplane) in a given training data set that separates samples of different categories.\n",
    "\n",
    "* SVM finds the decision boundary by maximizing the classification interval. The classification interval refers to the distance from the nearest feature sparse vector to the hyperplane. The goal of SVM is to find the hyperplane that maximizes the classification interval.\n",
    "\n",
    "* Feature sparse vectors on the boundary of the classification interval are called support vectors. When training an SVM, only the position and number of support vectors have an impact on the determination of the final classification hyperplane.\n",
    "\n",
    "* SVM can use kernel functions to map nonlinear problems into high-dimensional feature space, thereby finding a linear hyperplane in high-dimensional space. Commonly used kernel functions include linear kernels, polynomial kernels and radial basis function (RBF) kernels.\n",
    "\n",
    "* The classification problem of SVM can be transformed into a convex optimization problem. By minimizing the loss function, SVM finds an optimal hyperplane that maximizes the classification interval.\n",
    "\n",
    "SVM was originally designed for binary classification problems but can be extended to multi-class problems through some strategies. For example, one-to-one (OvO), $C(40,2)$ SVM classifiers will be constructed, each classifier solves the problem between two categories. One-to-One of the Rest (OvR),For each category, an SVM classifier is built that,distinguishes that category from all other categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylp3OsdYSL8a"
   },
   "source": [
    "## 评估模型性能\n",
    "\n",
    "正确标记/总样本数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5TeBEd6l8od"
   },
   "source": [
    "## Evaluate model performance\n",
    "\n",
    "Correctly labeled/total number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWI9a-Rsm5FZ"
   },
   "source": [
    "我原本选择了一个关于 Diabetic Retinopathy 的图片数据集。但每张图片的数据维度太大，平均图片大小为（680，1024，3）的彩色图片，我通过分析颜色通道确认了绿色通道更能表现血管结构特征。同时降采样了4倍得到了（173400，1）的铺平图片向量。但是由于血管过于细小，我无法再进行降采样，缩小数据维度。这就遇到了一个严重问题，如果要保证underdetermined，我们要用到超过173400个样本。数据过大，内存爆炸。\n",
    "因此我重新寻找了新的数据集，拥有40个分类，每个分类10个样本。原始数据维度为4096。而且这个数据集结构比血管得多，我可以更好的下采样。在后续的项目报告中我会提供两种字典的准确性分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijqVqsULpb4s"
   },
   "source": [
    "I originally chose an image dataset about Diabetic Retinopathy. However, the data dimension of each picture is too large, and the average picture size is (680, 1024, 3) color pictures. By analyzing the color channel, I confirmed that the green channel can better express the characteristics of the blood vessel structure. At the same time, 4 times downsampling is performed to obtain a flat image vector of (173400, 1). But because the blood vessels are too small, I can no longer downsample and reduce the data dimension. This encounters a serious problem. If we want to ensure underdetermined, we have to use more than 173,400 samples. The data is too large and the memory explodes.\n",
    "\n",
    "So I found a new data set with 40 categories and 10 samples in each category. The original data dimension is 4096. And this dataset is much more structured than blood vessels, so I can downsample better. I will provide an accuracy analysis of the two dictionaries in a subsequent project report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aS-z9Rvgu2L9"
   },
   "source": [
    "尊敬的老师们、亲爱的同学们，\n",
    "\n",
    "大家好！我很高兴能在这里向大家介绍我目前从事的项目，该项目涉及到基于字典学习的分类方法。我相信这个主题对老师和同学们都会很有趣。\n",
    "\n",
    "首先，让我详细介绍一下项目的背景和基本概念。我们所处理的数据属于某一大类别，例如人脸图像，被分为$k$个具体的个体类别。通过支持向量机（SVM），我们能够对任意未知类别的样本进行分类。\n",
    "\n",
    "**数据预处理**是我们项目的第一步，我们采用的是特征的中心化方法。这是通过以下公式实现的：\n",
    "\\[ \\vec{y}_t=\\vec{x}_t - \\frac{1}{N_t} \\sum_{t=1}^{N_t}\\vec{x}_t \\]\n",
    "\n",
    "这个过程的好处有两点：\n",
    "1. 可以去偏特征，有助于防止模型过度依赖特征的绝对值。\n",
    "2. 可以简化学习过程，使模型更容易捕捉数据模式。\n",
    "\n",
    "**算法逻辑**包括两个主要步骤：**字典学习**和**SVM分类**。\n",
    "\n",
    "1. **字典学习**：\n",
    "   - 通过使用训练集的数据矩阵进行字典学习，目的是通过稀疏表示向量学习一组基向量的矩阵。\n",
    "  \n",
    "2. **SVM分类**：\n",
    "   - 使用SVM训练特征矩阵，其中每个样本是一幅图像的稀疏表示向量，标签是图像所属的类别。\n",
    "   - 训练后，SVM模型学会了如何将稀疏编码映射到相应的类别。\n",
    "\n",
    "在**分类阶段**，对于测试集中的新图像，我们使用已学到的字典计算新图像的稀疏表示向量。然后，将该向量输入训练好的SVM模型进行分类预测，SVM将输出图像所属的类别。\n",
    "\n",
    "接下来，我们详细介绍了**字典的概念**，通过选择或学习字典，可以将给定的观察数据集稀疏表示为稀疏表示矩阵。\n",
    "\n",
    "关于**特征提取**，我们提到了两种获取字典的方法：通用基和定制基。通用基适用于要求稀疏表示的情况，而定制基则更能捕捉数据结构和特征。\n",
    "\n",
    "在**支持向量机（SVM）**的部分，我们详细解释了SVM的工作原理，包括在给定训练数据集中找到一个决策边界（超平面）来区分不同类别的样本。\n",
    "\n",
    "最后，我们提到了对模型性能进行**评估**的方法，即正确标记的样本数与总样本数的比值。考虑到原始图像数据集的尺寸问题，我选择了一个包含40个类别和每个类别10个样本的数据集进行实验，并在项目报告中提供了两种字典的准确性分析。\n",
    "\n",
    "总的来说，通过字典学习和SVM分类，我们能够有效地处理大规模的数据，并实现对未知类别的准确分类。这对于解决实际问题和提高模型的泛化能力具有重要意义。谢谢大家的聆听，如果有任何问题或建议，请随时提出。谢谢！"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
